#!/usr/bin/env python3
"""
Marcus GCP Neural Bridge for High-GPU OSINT Processing
Bridges local GNN OSINT analysis with Marcus high-GPU GCP infrastructure

Author: CTAS7 Intelligence Generator
Purpose: High-performance neural processing for large OSINT graphs
Integration: Marcus Neural Mux + GCP AI Platform + Vertex AI
"""

import asyncio
import json
import logging
import os
import aiohttp
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import google.cloud.aiplatform as aiplatform
from google.cloud import storage
from google.oauth2 import service_account

# Load environment variables
def load_env_file():
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value

load_env_file()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HighGPUJob:
    """High-GPU processing job"""
    job_id: str
    job_type: str
    graph_data: Dict[str, Any]
    model_config: Dict[str, Any]
    status: str  # submitted, running, completed, failed
    submitted_timestamp: datetime
    started_timestamp: Optional[datetime] = None
    completed_timestamp: Optional[datetime] = None
    results: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    gpu_instance: Optional[str] = None
    estimated_cost_usd: float = 0.0

@dataclass
class NeuralMuxStatus:
    """Neural Mux system status"""
    is_connected: bool
    total_services: int
    healthy_services: int
    gpu_availability: Dict[str, Any]
    queue_depth: int
    estimated_wait_time_minutes: int
    last_updated: datetime

class MarcusGCPNeuralBridge:
    """High-performance neural processing bridge for Marcus GCP infrastructure"""

    def __init__(self):
        # GCP configuration
        self.project_id = "gen-lang-client-0779767785"
        self.service_account_path = os.path.expanduser("~/.ctas7/marcus-gcp-key.json")
        self.bucket_name = f"{self.project_id}-neural-processing"
        self.region = "us-central1"

        # Neural Mux configuration
        self.neural_mux_url = "http://localhost:18100"
        self.marcus_agent_url = "http://localhost:18101"

        # GPU configurations
        self.gpu_configs = {
            "light": {
                "machine_type": "n1-standard-4",
                "gpu_type": "nvidia-tesla-t4",
                "gpu_count": 1,
                "estimated_cost_per_hour": 0.75
            },
            "medium": {
                "machine_type": "n1-standard-8",
                "gpu_type": "nvidia-tesla-v100",
                "gpu_count": 1,
                "estimated_cost_per_hour": 2.25
            },
            "heavy": {
                "machine_type": "n1-standard-16",
                "gpu_type": "nvidia-tesla-v100",
                "gpu_count": 4,
                "estimated_cost_per_hour": 9.00
            },
            "ultra": {
                "machine_type": "n1-standard-32",
                "gpu_type": "nvidia-tesla-a100",
                "gpu_count": 8,
                "estimated_cost_per_hour": 24.00
            }
        }

        # Initialize GCP clients
        self.credentials = None
        self.storage_client = None
        self.aiplatform_client = None

        self.initialize_gcp_clients()

        logger.info("ðŸ§  Marcus GCP Neural Bridge initialized")
        logger.info(f"   Project: {self.project_id}")
        logger.info(f"   Service Account: {os.path.basename(self.service_account_path)}")

    def initialize_gcp_clients(self):
        """Initialize Google Cloud Platform clients"""
        try:
            if os.path.exists(self.service_account_path):
                self.credentials = service_account.Credentials.from_service_account_file(
                    self.service_account_path
                )
                self.storage_client = storage.Client(
                    project=self.project_id,
                    credentials=self.credentials
                )

                # Initialize AI Platform
                aiplatform.init(
                    project=self.project_id,
                    location=self.region,
                    credentials=self.credentials
                )

                logger.info("âœ… GCP clients initialized successfully")

            else:
                logger.warning(f"Service account file not found: {self.service_account_path}")

        except Exception as e:
            logger.error(f"Failed to initialize GCP clients: {e}")

    async def get_neural_mux_status(self) -> NeuralMuxStatus:
        """Get Neural Mux system status"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.neural_mux_url}/status") as response:
                    if response.status == 200:
                        data = await response.json()

                        return NeuralMuxStatus(
                            is_connected=True,
                            total_services=data.get("total_services", 0),
                            healthy_services=data.get("healthy_services", 0),
                            gpu_availability=data.get("gpu_availability", {}),
                            queue_depth=data.get("queue_depth", 0),
                            estimated_wait_time_minutes=data.get("estimated_wait_time", 0),
                            last_updated=datetime.now()
                        )

        except Exception as e:
            logger.warning(f"Could not reach Neural Mux: {e}")

        return NeuralMuxStatus(
            is_connected=False,
            total_services=0,
            healthy_services=0,
            gpu_availability={},
            queue_depth=0,
            estimated_wait_time_minutes=999,
            last_updated=datetime.now()
        )

    async def estimate_processing_requirements(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate GPU processing requirements for graph data"""

        node_count = len(graph_data.get("nodes", []))
        edge_count = len(graph_data.get("edges", []))

        # Calculate complexity score
        complexity_score = (node_count * 0.1) + (edge_count * 0.05)

        # Determine GPU tier
        if complexity_score < 10:
            gpu_tier = "light"
            estimated_time_minutes = 2
        elif complexity_score < 50:
            gpu_tier = "medium"
            estimated_time_minutes = 5
        elif complexity_score < 200:
            gpu_tier = "heavy"
            estimated_time_minutes = 15
        else:
            gpu_tier = "ultra"
            estimated_time_minutes = 45

        config = self.gpu_configs[gpu_tier]
        estimated_cost = (estimated_time_minutes / 60) * config["estimated_cost_per_hour"]

        return {
            "complexity_score": complexity_score,
            "recommended_tier": gpu_tier,
            "estimated_time_minutes": estimated_time_minutes,
            "estimated_cost_usd": estimated_cost,
            "gpu_config": config,
            "node_count": node_count,
            "edge_count": edge_count
        }

    async def upload_graph_data_to_gcs(self, graph_data: Dict[str, Any], job_id: str) -> str:
        """Upload graph data to Google Cloud Storage"""
        try:
            # Ensure bucket exists
            bucket = self.storage_client.bucket(self.bucket_name)
            if not bucket.exists():
                bucket = self.storage_client.create_bucket(self.bucket_name, location=self.region)
                logger.info(f"Created GCS bucket: {self.bucket_name}")

            # Upload graph data
            blob_name = f"neural-jobs/{job_id}/graph_data.json"
            blob = bucket.blob(blob_name)

            blob.upload_from_string(
                json.dumps(graph_data, indent=2, default=str),
                content_type='application/json'
            )

            gcs_uri = f"gs://{self.bucket_name}/{blob_name}"
            logger.info(f"Uploaded graph data to: {gcs_uri}")

            return gcs_uri

        except Exception as e:
            logger.error(f"Failed to upload graph data: {e}")
            raise

    async def submit_high_gpu_job(self,
                                 graph_data: Dict[str, Any],
                                 job_type: str = "gnn_osint_analysis",
                                 gpu_tier: str = "auto") -> HighGPUJob:
        """Submit high-GPU processing job"""

        job_id = f"gnn-osint-{int(time.time())}"
        logger.info(f"ðŸš€ Submitting high-GPU job: {job_id}")

        # Estimate requirements
        requirements = await self.estimate_processing_requirements(graph_data)

        if gpu_tier == "auto":
            gpu_tier = requirements["recommended_tier"]

        logger.info(f"   GPU Tier: {gpu_tier}")
        logger.info(f"   Estimated Cost: ${requirements['estimated_cost_usd']:.2f}")
        logger.info(f"   Estimated Time: {requirements['estimated_time_minutes']} minutes")

        # Create job record
        job = HighGPUJob(
            job_id=job_id,
            job_type=job_type,
            graph_data=graph_data,
            model_config={
                "gpu_tier": gpu_tier,
                "input_dim": 512,
                "hidden_dim": 256,
                "num_layers": 5,
                "use_attention": True,
                "batch_size": 32
            },
            status="submitted",
            submitted_timestamp=datetime.now(),
            estimated_cost_usd=requirements["estimated_cost_usd"],
            gpu_instance=f"{gpu_tier}-{requirements['gpu_config']['gpu_type']}"
        )

        try:
            # Check Neural Mux availability
            mux_status = await self.get_neural_mux_status()

            if mux_status.is_connected and mux_status.queue_depth < 5:
                # Submit to Neural Mux for immediate processing
                result = await self.submit_to_neural_mux(job)
                if result.get("status") == "accepted":
                    job.status = "running"
                    job.started_timestamp = datetime.now()
                    logger.info("âœ… Job submitted to Neural Mux")
                    return job

            # Fall back to GCP AI Platform
            logger.info("Submitting to GCP AI Platform...")
            await self.submit_to_gcp_vertex_ai(job)

            return job

        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            logger.error(f"Job submission failed: {e}")
            raise

    async def submit_to_neural_mux(self, job: HighGPUJob) -> Dict[str, Any]:
        """Submit job to Neural Mux for processing"""
        payload = {
            "job_id": job.job_id,
            "job_type": job.job_type,
            "graph_data": job.graph_data,
            "model_config": job.model_config,
            "priority": "high" if job.model_config["gpu_tier"] in ["heavy", "ultra"] else "normal"
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.neural_mux_url}/submit_graph_job",
                    json=payload,
                    timeout=30
                ) as response:
                    return await response.json()

        except Exception as e:
            logger.error(f"Neural Mux submission failed: {e}")
            raise

    async def submit_to_gcp_vertex_ai(self, job: HighGPUJob):
        """Submit job to GCP Vertex AI for processing"""
        try:
            # Upload graph data to GCS
            gcs_uri = await self.upload_graph_data_to_gcs(job.graph_data, job.job_id)

            # Create Vertex AI training job
            gpu_config = self.gpu_configs[job.model_config["gpu_tier"]]

            training_job_spec = {
                "display_name": f"GNN-OSINT-{job.job_id}",
                "job_spec": {
                    "worker_pool_specs": [{
                        "machine_spec": {
                            "machine_type": gpu_config["machine_type"],
                            "accelerator_type": gpu_config["gpu_type"],
                            "accelerator_count": gpu_config["gpu_count"]
                        },
                        "replica_count": 1,
                        "container_spec": {
                            "image_uri": f"gcr.io/{self.project_id}/gnn-osint-processor:latest",
                            "env": [
                                {"name": "GRAPH_DATA_URI", "value": gcs_uri},
                                {"name": "JOB_ID", "value": job.job_id},
                                {"name": "MODEL_CONFIG", "value": json.dumps(job.model_config)}
                            ]
                        }
                    }]
                }
            }

            logger.info("Submitting to Vertex AI...")
            # Note: In production, this would use the actual Vertex AI API
            logger.info(f"Would submit job with spec: {training_job_spec}")

            job.status = "running"
            job.started_timestamp = datetime.now()

        except Exception as e:
            logger.error(f"Vertex AI submission failed: {e}")
            raise

    async def check_job_status(self, job_id: str) -> Dict[str, Any]:
        """Check status of high-GPU processing job"""
        try:
            # Check Neural Mux first
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.neural_mux_url}/job_status/{job_id}") as response:
                    if response.status == 200:
                        return await response.json()

            # Check GCP Vertex AI
            logger.info(f"Checking GCP job status for {job_id}")
            return {"status": "running", "progress": 50}  # Placeholder

        except Exception as e:
            logger.error(f"Error checking job status: {e}")
            return {"status": "unknown", "error": str(e)}

    async def get_job_results(self, job_id: str) -> Dict[str, Any]:
        """Get results from completed high-GPU job"""
        try:
            # Try Neural Mux first
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.neural_mux_url}/job_results/{job_id}") as response:
                    if response.status == 200:
                        return await response.json()

            # Try GCS for Vertex AI results
            bucket = self.storage_client.bucket(self.bucket_name)
            results_blob = bucket.blob(f"neural-jobs/{job_id}/results.json")

            if results_blob.exists():
                results_data = json.loads(results_blob.download_as_text())
                return results_data

            return {"status": "not_found", "error": "Job results not available"}

        except Exception as e:
            logger.error(f"Error getting job results: {e}")
            return {"status": "error", "error": str(e)}

    async def process_graph_with_high_gpu(self,
                                        graph_data: Dict[str, Any],
                                        wait_for_completion: bool = True,
                                        max_wait_minutes: int = 60) -> Dict[str, Any]:
        """Complete high-GPU graph processing workflow"""
        logger.info("ðŸ§  Starting high-GPU graph processing workflow...")

        # Submit job
        job = await self.submit_high_gpu_job(graph_data)

        if not wait_for_completion:
            return {
                "status": "submitted",
                "job_id": job.job_id,
                "estimated_completion": datetime.now() + timedelta(minutes=15)
            }

        # Wait for completion
        start_time = time.time()
        max_wait_seconds = max_wait_minutes * 60

        while time.time() - start_time < max_wait_seconds:
            status = await self.check_job_status(job.job_id)

            if status.get("status") == "completed":
                logger.info(f"âœ… Job {job.job_id} completed successfully")
                results = await self.get_job_results(job.job_id)
                return {
                    "status": "success",
                    "job_id": job.job_id,
                    "results": results,
                    "processing_time_minutes": (time.time() - start_time) / 60,
                    "cost_usd": job.estimated_cost_usd
                }

            elif status.get("status") == "failed":
                logger.error(f"âŒ Job {job.job_id} failed: {status.get('error')}")
                return {
                    "status": "failed",
                    "job_id": job.job_id,
                    "error": status.get("error")
                }

            # Wait before checking again
            await asyncio.sleep(30)
            logger.info(f"â³ Job {job.job_id} still running... ({status.get('progress', 0)}% complete)")

        # Timeout
        logger.warning(f"â° Job {job.job_id} timed out after {max_wait_minutes} minutes")
        return {
            "status": "timeout",
            "job_id": job.job_id,
            "message": f"Job did not complete within {max_wait_minutes} minutes"
        }

    async def get_cost_estimate(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """Get cost estimate for processing graph data"""
        requirements = await self.estimate_processing_requirements(graph_data)

        estimates = {}
        for tier, config in self.gpu_configs.items():
            time_multiplier = 1.0
            if tier == "light":
                time_multiplier = 2.0
            elif tier == "medium":
                time_multiplier = 1.5
            elif tier == "heavy":
                time_multiplier = 1.0
            elif tier == "ultra":
                time_multiplier = 0.7

            estimated_time = requirements["estimated_time_minutes"] * time_multiplier
            estimated_cost = (estimated_time / 60) * config["estimated_cost_per_hour"]

            estimates[tier] = {
                "estimated_time_minutes": estimated_time,
                "estimated_cost_usd": estimated_cost,
                "gpu_config": config
            }

        return {
            "graph_complexity": requirements["complexity_score"],
            "recommended_tier": requirements["recommended_tier"],
            "cost_estimates": estimates
        }

# Configuration for Marcus GCP Bridge
MARCUS_GCP_CONFIG = {
    "project_id": "gen-lang-client-0779767785",
    "service_account_path": "~/.ctas7/marcus-gcp-key.json",
    "neural_mux_url": "http://localhost:18100",
    "default_gpu_tier": "medium",
    "max_job_wait_minutes": 60,
    "auto_fallback": True
}

async def main():
    """Demo of Marcus GCP Neural Bridge"""
    logger.info("ðŸ§  Marcus GCP Neural Bridge Demo")
    logger.info("=" * 50)

    bridge = MarcusGCPNeuralBridge()

    # Check Neural Mux status
    status = await bridge.get_neural_mux_status()
    logger.info(f"Neural Mux Status: {'Connected' if status.is_connected else 'Offline'}")

    if status.is_connected:
        logger.info(f"Services: {status.healthy_services}/{status.total_services} healthy")
        logger.info(f"Queue Depth: {status.queue_depth}")

    # Demo graph data
    demo_graph = {
        "nodes": [{"id": f"node_{i}", "type": "entity"} for i in range(100)],
        "edges": [{"src": f"node_{i}", "dst": f"node_{i+1}"} for i in range(99)]
    }

    # Get cost estimate
    cost_estimate = await bridge.get_cost_estimate(demo_graph)
    logger.info(f"\nCost Estimate:")
    logger.info(f"Complexity: {cost_estimate['graph_complexity']:.2f}")
    logger.info(f"Recommended: {cost_estimate['recommended_tier']}")

    for tier, estimate in cost_estimate['cost_estimates'].items():
        logger.info(f"  {tier}: ${estimate['estimated_cost_usd']:.2f} (~{estimate['estimated_time_minutes']:.1f} min)")

    logger.info("\nâœ… Marcus GCP Neural Bridge ready for high-GPU processing")

if __name__ == "__main__":
    asyncio.run(main())